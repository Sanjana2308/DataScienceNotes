# Week 3

## Introduction to Apache Spark
- Introduction to Big Data and Spark
- Spark's Basic Architecture
- Cluster Manager, Spark Session, Spark Context, Driver Node, Worker Node
- Client Mode and Cluster Mode
- Spark Toolset - Low level API (RDD), Datasets, DataFrame, SQL
- Spark Components - Spark Streaming, Mlib, Graph Processing, DAG, Lazy Evulation, Transformations and Actions, Spark Memory management

## PySpark Programming

- Introduction to PySpark and setting up the environment
- Hands-on Exercise: Writing basic PySpark programs
- Hands-on Exercise: Analyzing a sample dataset using PySpark
<br>
<br>

- RDDs and transformations in PySpark
- Hands-on Exercise: Transforming data with PySpark RDDs
<br>
<br>

- DataFrames in PySpark
- Creating Dask DataFrames in PythonCreating DataFrames
- Looking at Data in DataFrames
- Selecting, Renaming, Filtering Data in a Pandas DataFrame
- Manipulating, Droping, Sorting, Aggregations, Joining, GroupeBy  DataFrames
- Updating Data, Handling Null values, Na functions, Built in functions, When otherwise, Working on complex JSON files, Union and Union ALL, Window Functions, Date and time Functions, Built in functions,  in a DataFrame
- Applying Functions in a Pandas DataFrame
- Pandas and Alternatives
- Hands-on Exercise: Working with DataFrames
- Creating Widgets

<br>
<br>

- PySpark - Ingestion of CSV, simple and complex JSON files into the data lake as parquet files/ tables.
- PySpark - Transformations such as Filter, Join, Simple Aggregations, GroupBy, Window functions etc.
- PySpark - Creating local and temporary views
- Spark SQL - Creating databases, tables and views
- Spark SQL - Transformations such as Filter, Join, Simple Aggregations, GroupBy, Window functions etc.
- Spark SQL - Creating local and temporary views
- Implementing full refresh and incremental load patterns using partitions
- Hands-on: Processing JSON and CSV data with PySpark
- Hands-on: ETL (Extract, Transform, Load) with PySpark

## Assignments


## Spark and PySpark Coding Assessment

