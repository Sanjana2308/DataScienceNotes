# Coding Challenge

## Dataset: E-commerce Transactions
This dataset contains information about e-commerce transactions, including details
about the products purchased, the customer who made the purchase, the price, and the
transaction date.

### Sample Data:
```csv
transaction_id,customer_id,product,category,price,quantity,discount_percentage,transaction_date
1,101,Laptop,Electronics,1000,1,10,2023-08-01
2,102,Smartphone,Electronics,700,2,5,2023-08-01
3,103,Shirt,Fashion,40,3,0,2023-08-02
4,104,Blender,Home Appliance,150,1,15,2023-08-03
5,101,Headphones,Electronics,100,2,10,2023-08-03
6,105,Shoes,Fashion,60,1,20,2023-08-04
7,106,Refrigerator,Home Appliance,800,1,25,2023-08-05
8,107,Book,Books,20,4,0,2023-08-05
9,108,Toaster,Home Appliance,30,1,5,2023-08-06
10,102,Tablet,Electronics,300,2,10,2023-08-06
```

### Exercises:
**Setting up the environment:**
```python
import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('Practice').getOrCreate()

file_name = '/content/sample_data/1_ecommerce_transactions.csv'
df = spark.read.csv(file_name, header=True, inferSchema=True)
df.show()
```

1. **Calculate the Total Revenue per Category**
- Group the data by category and calculate the total revenue generated by each category. (Hint: Multiply price by quantity and apply the discount to get the actual revenue.)
```python
df_new = df.withColumn('revenue', df['price'] * df['quantity'] * (1 - df['discount_percentage']/100))
total_revenue_by_category = df_new.groupBy('category').sum('revenue').withColumnRenamed('sum(revenue)', 'total_revenue')
total_revenue_by_category.show()
```

2. **Filter Transactions with a Discount Greater Than 10%**
- Filter the dataset to show only transactions where the discount percentage is greater than 10%.
```python
filtered_transactions = df.filter(df['discount_percentage'] > 10)
filtered_transactions.show()
```

3. **Find the Most Expensive Product Sold**
- Identify the product with the highest individual price.
```python
most_expensive_product = df.orderBy(df['price'].desc()).limit(1)
most_expensive_product.show()
```

4. **Calculate the Average Quantity of Products Sold per Category**
- Group the data by category and calculate the average quantity of products sold in each category.
```python
average_quantity_by_category = df.groupBy('category').avg('quantity').withColumnRenamed('avg(quantity)', 'average_quantity')
average_quantity_by_category.show()
```

5. **Identify Customers Who Purchased More Than One Product**
- Filter the data to show only customers who purchased more than one product in a single transaction.
```python
customers_multiple_purchases = df.groupBy('customer_id').count().filter('count > 1')
customers_multiple_purchases.show()
```

6. **Find the Top 3 Highest Revenue Transactions**
- Calculate the total revenue for each transaction and identify the top 3 highest revenue transactions.
```python
top_3_transactions = df_new.orderBy(df_new['revenue'].desc()).limit(3)
top_3_transactions.show()
```

7. **Calculate the Total Number of Transactions per Day**
- Group the data by transaction_date and calculate the total number of transactions for each day.
```python
transactions_per_day = df.groupBy('transaction_date').count().withColumnRenamed('count', 'total_transactions')
transactions_per_day.show()
```

8. **Find the Customer Who Spent the Most Money**
- Calculate the total amount spent by each customer and identify the customer with the highest total spending.
```python
total_spending_by_customer = df_new.groupBy('customer_id').sum('revenue').withColumnRenamed('sum(revenue)', 'total_spending')
customer_most_spending = total_spending_by_customer.orderBy(total_spending_by_customer['total_spending'].desc()).limit(1)
customer_most_spending.show()
```

9. **Calculate the Average Discount Given per Product Category**
- Group the data by category and calculate the average discount percentage applied to products in each category.
```python
average_discount_by_category = df.groupBy('category').avg('discount_percentage').withColumnRenamed('avg(discount_percentage)', 'average_discount')
average_discount_by_category.show()
```

10. **Create a New Column for Final Price After Discount**
- Add a new column final_price that calculates the total price after applying the discount ( price - (price * discount_percentage / 100) ).
```python
df = df.withColumn('final_price', df['price'] - (df['price'] * df['discount_percentage']/100))
df.show()
```


## Dataset: Banking Transactions
This dataset contains information about customer transactions at a bank. Each row
represents a transaction, including the transaction ID, customer ID, transaction type,
amount, and date.

### Sample Data:
```csv
transaction_id,customer_id,transaction_type,amount,transaction_date
1,201,Deposit,5000,2023-09-01
2,202,Withdrawal,2000,2023-09-01
3,203,Deposit,3000,2023-09-02
4,201,Withdrawal,1500,2023-09-02
5,204,Deposit,10000,2023-09-03
6,205,Withdrawal,500,2023-09-03
7,202,Deposit,2500,2023-09-04
8,206,Withdrawal,700,2023-09-04
9,203,Deposit,4000,2023-09-05
10,204,Withdrawal,3000,2023-09-05
```

### Exercises:

**Setting up the environment:**
```python
import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('Practice').getOrCreate()

file_name = '/content/sample_data/2_banking_transactions.csv'
df = spark.read.csv(file_name, header=True, inferSchema=True)
df.show()
```

1. **Calculate the Total Deposit and Withdrawal Amounts**
- Group the data by transaction_type and calculate the total amounts for both deposits and withdrawals.
```python
total_amount = df.groupBy('transaction_type').agg({'amount': 'sum'}).show()
```

2. **Filter Transactions Greater Than $3,000**
- Filter the dataset to show only transactions where the amount is greater than $3,000.
```python
greater_than_3000 = df.filter(df['amount'] > 3000)
greater_than_3000.show()
```

3. **Find the Largest Deposit Made**
- Identify the transaction with the highest deposit amount.
```python
largest_deposit = df.orderBy('amount', ascending=False).limit(1)
largest_deposit.show()
```

4. **Calculate the Average Transaction Amount for Each Transaction Type**
- Group the data by transaction_type and calculate the average amount for deposits and withdrawals.
```python
average_transaction = df.groupBy('transaction_type').agg({'amount': 'avg'})
average_transaction.show()
```

5. **Find Customers Who Made Both Deposits and Withdrawals**
- Identify customers who have made at least one deposit and one withdrawal.
```python
deposit_customers = df.filter(df['transaction_type'] == 'Deposit').select('customer_id').distinct()
withdrawal_customers = df.filter(df['transaction_type'] == 'Withdrawal').select('customer_id').distinct()

deposit_customers.intersect(withdrawal_customers).show()
```

6. **Calculate the Total Amount of Transactions per Day**
- Group the data by transaction_date and calculate the total amount of all transactions for each day.
```python
total_amount_per_day = df.groupBy('transaction_date').agg({'amount': 'sum'})
total_amount_per_day.show()
```

7. **Find the Customer with the Highest Total Withdrawal**
- Calculate the total amount withdrawn by each customer and identify the customer with the highest total withdrawal.
```python
df_new = df.filter(df['transaction_type'] == 'Withdrawal').groupBy('customer_id').agg({'amount': 'sum'})
total_withdrawal = df_new.orderBy('sum(amount)', ascending=False).limit(1)
total_withdrawal.show()
```

8. **Calculate the Number of Transactions for Each Customer**
- Group the data by customer_id and calculate the total number of transactions made by each customer.
```python
transactions_per_customer = df.groupBy('customer_id').count()
transactions_per_customer.show()
```

9. **Find All Transactions That Occurred on the Same Day as a Withdrawal Greater Than $1,000**
- Filter the data to show all transactions that occurred on the same day as a withdrawal of more than $1,000.
```python
df_new = df.filter((df['transaction_type'] == 'Withdrawal') & (df['amount'] > 1000))
df_new.show()
```

10. **Create a New Column to Classify Transactions as "High" or "Low" Value**
- Add a new column transaction_value that classifies a transaction as "High" if the amount is greater than $5,000, otherwise classify it as "Low."
```python
from pyspark.sql.functions import when

df = df.withColumn('transaction_value', when(df['amount'] > 5000, 'High').otherwise('Low'))
df.show()
```

## Dataset: Health & Fitness Tracker Data
This dataset contains information about users' daily health and fitness activities,
including steps taken, calories burned, hours of sleep, and workout types.

### Sample Data:
```csv
user_id,date,steps,calories_burned,hours_of_sleep,workout_type
1,2023-09-01,12000,500,7,Cardio
2,2023-09-01,8000,400,6.5,Strength
3,2023-09-01,15000,650,8,Yoga
1,2023-09-02,10000,450,6,Cardio
2,2023-09-02,9500,500,7,Cardio
3,2023-09-02,14000,600,7.5,Strength
1,2023-09-03,13000,550,8,Yoga
2,2023-09-03,12000,520,6.5,Yoga
3,2023-09-03,16000,700,7,Cardio
```

### Exercises:
**Setting up the environment:**
```python
import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('Practice').getOrCreate()

file_name = '/content/sample_data/3_health_and_fitness_tracker.csv'
df = spark.read.csv(file_name, header=True, inferSchema=True)
df.show()
```

1. **Find the Total Steps Taken by Each User**
- Group the data by user_id and calculate the total steps taken by each user across all days.
```python
from pyspark.sql.functions import sum

total_steps_by_user = df.groupBy('user_id').agg(sum('steps').alias('total_steps'))
total_steps_by_user.show()
```

2. **Filter Days with More Than 10,000 Steps**
- Filter the dataset to show only the days where the user took more than 10,000 steps.
```python
days_with_more_than_10k_steps = df.filter(df['steps'] > 10000)
days_with_more_than_10k_steps.show()
```

3. **Calculate the Average Calories Burned by Workout Type**
- Group the data by workout_type and calculate the average calories burned for each workout type.
```python
from pyspark.sql.functions import avg

average_calories_by_workout = df.groupBy('workout_type').agg(avg('calories_burned').alias('average_calories'))
average_calories_by_workout.show()
```

4. **Identify the Day with the Most Steps for Each User**
- For each user, find the day when they took the most steps.
```python
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number

window = Window.partitionBy('user_id').orderBy(df['steps'].desc())

most_steps_for_each_user = df.withColumn('rank', row_number().over(window)).filter('rank == 1').select('user_id', 'date', 'steps')
most_steps_for_each_user.show()
```

5. **Find Users Who Burned More Than 600 Calories on Any Day**
- Filter the data to show only the users who burned more than 600 calories on any day.
```python
users_with_more_than_600_calories = df.filter(df['calories_burned'] > 600).select('user_id', 'date', 'calories_burned').distinct()
users_with_more_than_600_calories.show()
```

6. **Calculate the Average Hours of Sleep per User**
- Group the data by user_id and calculate the average hours of sleep for each user.
```python
average_hours_of_sleep_per_user = df.groupBy('user_id').agg(avg('hours_of_sleep').alias('average_hours_of_sleep'))
average_hours_of_sleep_per_user.show()
```

7. **Find the Total Calories Burned per Day**
- Group the data by date and calculate the total calories burned by all users combined for each day.
```python
total_calories_burned_per_day = df.groupBy('date').agg(sum('calories_burned').alias('total_calories_burned'))
total_calories_burned_per_day.show()
```

8. **Identify Users Who Did Different Types of Workouts**
- Identify users who participated in more than one type of workout.
```python
users_do_diff_workouts = df.select('user_id', 'workout_type').distinct().groupBy('user_id').count().filter('count > 1')
users_do_diff_workouts.show()
```

9. **Calculate the Total Number of Workouts per User**
- Group the data by user_id and count the total number of workouts completed by each user.
```python
total_workouts_per_user = df.groupBy('user_id').count().withColumnRenamed('count', 'total_workouts')
total_workouts_per_user.show()
```

10. **Create a New Column for "Active" Days**
- Add a new column called active_day that classifies a day as "Active" if the user took more than 10,000 steps, otherwise classify it as "Inactive."
```python
from pyspark.sql.functions import when

df = df.withColumn('active_day', when(df['steps'] > 10000, 'Active').otherwise('Inactive'))
df.show()
```

## Dataset: Music Streaming Data
This dataset contains information about users' music streaming habits, including the
song title, artist, duration of the song, streaming time, and user's location.

### Sample Data:
```csv
user_id,song_title,artist,duration_seconds,streaming_time,location
1,Blinding Lights,The Weeknd,200,2023-09-01 08:15:00,New York
2,Shape of You,Ed Sheeran,240,2023-09-01 09:20:00,Los Angeles
3,Levitating,Dua Lipa,180,2023-09-01 10:30:00,London
1,Starboy,The Weeknd,220,2023-09-01 11:00:00,New York
2,Perfect,Ed Sheeran,250,2023-09-01 12:15:00,Los Angeles
3,Don't Start Now,Dua Lipa,200,2023-09-02 08:10:00,London
1,Save Your Tears,The Weeknd,210,2023-09-02 09:00:00,New York
2,Galway Girl,Ed Sheeran,190,2023-09-02 10:00:00,Los Angeles
3,New Rules,Dua Lipa,230,2023-09-02 11:00:00,London
```

### Exercises:
**Setting up the environment:**
```python
import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('Practice').getOrCreate()

file_name = '/content/sample_data/4_music_streaming.csv'
df = spark.read.csv(file_name, header=True, inferSchema=True)
df.show()
```

**1. Calculate the Total Listening Time for Each User**
- Group the data by user_id and calculate the total time spent streaming (in seconds) for each user.
```python
total_listening_time = df.groupBy('user_id').agg({'duration_seconds': 'sum'}).withColumnRenamed('sum(duration_seconds)', 'total_listening_time')
total_listening_time.show()
```

**2. Filter Songs Streamed for More Than 200 Seconds**
- Filter the dataset to show only the songs where the duration_seconds is greater than 200.
```python
songs_streamed_more_than_200 = df.filter(df['duration_seconds'] > 200)
songs_streamed_more_than_200.show()
```

**3. Find the Most Popular Artist (by Total Streams)**
- Group the data by artist and find the artist with the most streams (i.e., the highest number of song plays).
```python
most_popular_artist = df.groupBy('artist').count().withColumnRenamed('count', 'total_streams').orderBy('total_streams', ascending=False).limit(1)
most_popular_artist.show()
```

**4. Identify the Song with the Longest Duration**
- Identify the song with the longest duration in the dataset.
```python
song_with_longest_duration = df.orderBy('duration_seconds', ascending=False).limit(1)
song_with_longest_duration.show()
```

**5. Calculate the Average Song Duration by Artist**
- Group the data by artist and calculate the average song duration for each artist.
```python
average_song_duration_by_artist = df.groupBy('artist').agg({'duration_seconds': 'avg'}).withColumnRenamed('avg(duration_seconds)', 'average_song_duration')
average_song_duration_by_artist.show()
```

**6. Find the Top 3 Most Streamed Songs per User**
- For each user, find the top 3 most-streamed songs (i.e., songs they played most frequently).
```python
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number

window_spec = Window.partitionBy('user_id').orderBy(df['duration_seconds'].desc())
df_with_rank = df.withColumn('rank', row_number().over(window_spec))
top_3_songs_per_user = df_with_rank.filter(df_with_rank['rank'] <= 3)
top_3_songs_per_user.show()
```

**7. Calculate the Total Number of Streams per Day**
- Group the data by streaming_time (by extracting the date) and calculate the total number of streams for each day.
```python
from pyspark.sql.functions import to_date

df_with_date = df.withColumn('streaming_date', to_date(df['streaming_time']))
total_streams_per_day = df_with_date.groupBy('streaming_date').count().withColumnRenamed('count', 'total_streams')
total_streams_per_day.show()
```

**8. Identify Users Who Streamed Songs from More Than One Artist**
- Find users who listened to songs by more than one artist.
```python
from pyspark.sql.functions import countDistinct

users_with_multiple_artists = df.groupBy('user_id').agg(countDistinct('artist').alias('distinct_artists')).filter('distinct_artists > 1')
users_with_multiple_artists.show()
```

**9. Calculate the Total Streams for Each Location**
- Group the data by location and calculate the total number of streams for each location.
```python
total_streams_per_location = df.groupBy('location').count().withColumnRenamed('count', 'total_streams')
total_streams_per_location.show()
```

**10. Create a New Column to Classify Long and Short Songs**
- Add a new column song_length that classifies a song as "Long" if duration_seconds > 200 , otherwise classify it as "Short."
```python
from pyspark.sql.functions import when

df_with_length = df.withColumn('song_length', when(df['duration_seconds'] > 200, 'Long').otherwise('Short'))
df_with_length.show()
```

## Dataset: Retail Store Sales Data
This dataset contains information about sales transactions at a retail store,
including the product name, category, price, quantity sold, and sales date.

### Sample Data:
```csv
transaction_id,product_name,category,price,quantity,sales_date
1,Apple,Groceries,0.50,10,2023-09-01
2,T-shirt,Clothing,15.00,2,2023-09-01
3,Notebook,Stationery,2.00,5,2023-09-02
4,Banana,Groceries,0.30,12,2023-09-02
5,Laptop,Electronics,800.00,1,2023-09-03
6,Pants,Clothing,25.00,3,2023-09-03
7,Headphones,Electronics,100.00,2,2023-09-04
8,Pen,Stationery,1.00,10,2023-09-04
9,Orange,Groceries,0.60,8,2023-09-05
10,Sneakers,Clothing,50.00,1,2023-09-05
```

### Exercises:
**Cleaning up the environment:**
```python
import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('Practice').getOrCreate()

file_name = '/content/sample_data/5_retail_store_sales.csv'
df = spark.read.csv(file_name, header=True, inferSchema=True)
df.show()
```

**1. Calculate the Total Revenue per Category**
- Group the data by category and calculate the total revenue generated by each category. (Hint:Multiply price by quantity for each transaction.)
```python
from pyspark.sql.functions import col, sum

df_new = df.withColumn('total_revenue', col('price') * col('quantity'))

total_revenue_per_category = df_new.groupBy('category').agg(sum('total_revenue').alias('total_revenue'))
total_revenue_per_category.show()
```

**2. Filter Transactions Where the Total Sales Amount is Greater Than $100**
- Filter the dataset to show only transactions where the total sales amount (price * quantity) is greater than $100.
```python
filtered_transactions = df_new.filter(col('total_revenue') > 100)
filtered_transactions.show()
```

**3. Find the Most Sold Product**
- Identify the product with the highest total quantity sold across all transactions.
```python
most_sold_product = df.groupBy('product_name').agg(sum('quantity').alias('total_quantity'))
most_sold_product = most_sold_product.orderBy(col('total_quantity').desc()).limit(1)
most_sold_product.show()
```

**4. Calculate the Average Price per Product Category**
- Group the data by category and calculate the average price of products in each category.
```python
from pyspark.sql.functions import avg

average_price_per_category = df.groupBy('category').agg(avg('price').alias('average_price'))
average_price_per_category.show()
```

**5. Find the Top 3 Highest Grossing Products**
- Calculate the total revenue for each product and identify the top 3 products that generated the most revenue.
```python
total_revenue_per_product = df_new.groupBy('product_name').agg(sum('total_revenue').alias('total_revenue'))
top_3_highest_grossing = total_revenue_per_product.orderBy(col('total_revenue').desc()).limit(3)
top_3_highest_grossing.show()
```

**6. Calculate the Total Number of Items Sold per Day**
- Group the data by sales_date and calculate the total quantity of items sold for each day.
```python
from pyspark.sql.functions import count

total_items_sold_per_day = df.groupBy('sales_date').agg(sum('quantity').alias('total_quantity'))
total_items_sold_per_day.show()
```

**7. Identify the Product with the Lowest Price in Each Category**
- For each category, identify the product with the lowest price.
```python
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number

window_spec = Window.partitionBy('category').orderBy('price')
lowest_price_per_category = df.withColumn('row_num', row_number().over(window_spec))
lowest_price_per_category = lowest_price_per_category.filter(col('row_num') == 1).drop('row_num')
lowest_price_per_category.show()
```

**8. Calculate the Total Revenue for Each Product**
- Group the data by product_name and calculate the total revenue generated by each product.
```python
total_revenue_per_product = df_new.groupBy('product_name').agg(sum('total_revenue').alias('total_revenue'))
total_revenue_per_product.show()
```

**9. Find the Total Sales per Day for Each Category**
- Group the data by sales_date and category to calculate the total sales for each category per day.
```python
from pyspark.sql.functions import sum

total_sales_per_day_per_category = df.groupBy('sales_date', 'category').agg(sum('quantity').alias('total_quantity'))
total_sales_per_day_per_category.show()
```

**10. Create a New Column for Discounted Price**
- Add a new column called discounted_price that applies a 10% discount to the original price for each product ( price * 0.9 ).
```python
from pyspark.sql.functions import col

df = df.withColumn('discounted_price', col('price') * 0.9)
df.show()
```
