# Coding Challenge

## Dataset: E-commerce Transactions
This dataset contains information about e-commerce transactions, including details
about the products purchased, the customer who made the purchase, the price, and the
transaction date.

### Sample Data:
```csv
transaction_id,customer_id,product,category,price,quantity,discount_percentage,transaction_date
1,101,Laptop,Electronics,1000,1,10,2023-08-01
2,102,Smartphone,Electronics,700,2,5,2023-08-01
3,103,Shirt,Fashion,40,3,0,2023-08-02
4,104,Blender,Home Appliance,150,1,15,2023-08-03
5,101,Headphones,Electronics,100,2,10,2023-08-03
6,105,Shoes,Fashion,60,1,20,2023-08-04
7,106,Refrigerator,Home Appliance,800,1,25,2023-08-05
8,107,Book,Books,20,4,0,2023-08-05
9,108,Toaster,Home Appliance,30,1,5,2023-08-06
10,102,Tablet,Electronics,300,2,10,2023-08-06
```

### Exercises:
**Setting up the environment:**
```python
import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('Practice').getOrCreate()

file_name = '/content/sample_data/1_ecommerce_transactions.csv'
df = spark.read.csv(file_name, header=True, inferSchema=True)
df.show()
```

1. **Calculate the Total Revenue per Category**
- Group the data by category and calculate the total revenue generated by each category. (Hint: Multiply price by quantity and apply the discount to get the actual revenue.)
```python
df_new = df.withColumn('revenue', df['price'] * df['quantity'] * (1 - df['discount_percentage']/100))
total_revenue_by_category = df_new.groupBy('category').sum('revenue').withColumnRenamed('sum(revenue)', 'total_revenue')
total_revenue_by_category.show()
```

2. **Filter Transactions with a Discount Greater Than 10%**
- Filter the dataset to show only transactions where the discount percentage is greater than 10%.
```python
filtered_transactions = df.filter(df['discount_percentage'] > 10)
filtered_transactions.show()
```

3. **Find the Most Expensive Product Sold**
- Identify the product with the highest individual price.
```python
most_expensive_product = df.orderBy(df['price'].desc()).limit(1)
most_expensive_product.show()
```

4. **Calculate the Average Quantity of Products Sold per Category**
- Group the data by category and calculate the average quantity of products sold in each category.
```python
average_quantity_by_category = df.groupBy('category').avg('quantity').withColumnRenamed('avg(quantity)', 'average_quantity')
average_quantity_by_category.show()
```

5. **Identify Customers Who Purchased More Than One Product**
- Filter the data to show only customers who purchased more than one product in a single transaction.
```python
customers_multiple_purchases = df.groupBy('customer_id').count().filter('count > 1')
customers_multiple_purchases.show()
```

6. **Find the Top 3 Highest Revenue Transactions**
- Calculate the total revenue for each transaction and identify the top 3 highest revenue transactions.
```python
top_3_transactions = df_new.orderBy(df_new['revenue'].desc()).limit(3)
top_3_transactions.show()
```

7. **Calculate the Total Number of Transactions per Day**
- Group the data by transaction_date and calculate the total number of transactions for each day.
```python
transactions_per_day = df.groupBy('transaction_date').count().withColumnRenamed('count', 'total_transactions')
transactions_per_day.show()
```

8. **Find the Customer Who Spent the Most Money**
- Calculate the total amount spent by each customer and identify the customer with the highest total spending.
```python
total_spending_by_customer = df_new.groupBy('customer_id').sum('revenue').withColumnRenamed('sum(revenue)', 'total_spending')
customer_most_spending = total_spending_by_customer.orderBy(total_spending_by_customer['total_spending'].desc()).limit(1)
customer_most_spending.show()
```

9. **Calculate the Average Discount Given per Product Category**
- Group the data by category and calculate the average discount percentage applied to products in each category.
```python
average_discount_by_category = df.groupBy('category').avg('discount_percentage').withColumnRenamed('avg(discount_percentage)', 'average_discount')
average_discount_by_category.show()
```

10. **Create a New Column for Final Price After Discount**
- Add a new column final_price that calculates the total price after applying the discount ( price - (price * discount_percentage / 100) ).
```python
df = df.withColumn('final_price', df['price'] - (df['price'] * df['discount_percentage']/100))
df.show()
```


## Dataset: Banking Transactions
This dataset contains information about customer transactions at a bank. Each row
represents a transaction, including the transaction ID, customer ID, transaction type,
amount, and date.

### Sample Data:
```csv
transaction_id,customer_id,transaction_type,amount,transaction_date
1,201,Deposit,5000,2023-09-01
2,202,Withdrawal,2000,2023-09-01
3,203,Deposit,3000,2023-09-02
4,201,Withdrawal,1500,2023-09-02
5,204,Deposit,10000,2023-09-03
6,205,Withdrawal,500,2023-09-03
7,202,Deposit,2500,2023-09-04
8,206,Withdrawal,700,2023-09-04
9,203,Deposit,4000,2023-09-05
10,204,Withdrawal,3000,2023-09-05
```

### Exercises:

**Setting up the environment:**
```python
import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('Practice').getOrCreate()

file_name = '/content/sample_data/2_banking_transactions.csv'
df = spark.read.csv(file_name, header=True, inferSchema=True)
df.show()
```

1. **Calculate the Total Deposit and Withdrawal Amounts**
- Group the data by transaction_type and calculate the total amounts for both deposits and withdrawals.
```python
total_amount = df.groupBy('transaction_type').agg({'amount': 'sum'}).show()
```

2. **Filter Transactions Greater Than $3,000**
- Filter the dataset to show only transactions where the amount is greater than $3,000.
```python
greater_than_3000 = df.filter(df['amount'] > 3000)
greater_than_3000.show()
```

3. **Find the Largest Deposit Made**
- Identify the transaction with the highest deposit amount.
```python
largest_deposit = df.orderBy('amount', ascending=False).limit(1)
largest_deposit.show()
```

4. **Calculate the Average Transaction Amount for Each Transaction Type**
- Group the data by transaction_type and calculate the average amount for deposits and withdrawals.
```python
average_transaction = df.groupBy('transaction_type').agg({'amount': 'avg'})
average_transaction.show()
```

5. **Find Customers Who Made Both Deposits and Withdrawals**
- Identify customers who have made at least one deposit and one withdrawal.
```python
deposit_customers = df.filter(df['transaction_type'] == 'Deposit').select('customer_id').distinct()
withdrawal_customers = df.filter(df['transaction_type'] == 'Withdrawal').select('customer_id').distinct()

deposit_customers.intersect(withdrawal_customers).show()
```

6. **Calculate the Total Amount of Transactions per Day**
- Group the data by transaction_date and calculate the total amount of all transactions for each day.
```python
total_amount_per_day = df.groupBy('transaction_date').agg({'amount': 'sum'})
total_amount_per_day.show()
```

7. **Find the Customer with the Highest Total Withdrawal**
- Calculate the total amount withdrawn by each customer and identify the customer with the highest total withdrawal.
```python
df_new = df.filter(df['transaction_type'] == 'Withdrawal').groupBy('customer_id').agg({'amount': 'sum'})
total_withdrawal = df_new.orderBy('sum(amount)', ascending=False).limit(1)
total_withdrawal.show()
```

8. **Calculate the Number of Transactions for Each Customer**
- Group the data by customer_id and calculate the total number of transactions made by each customer.
```python
transactions_per_customer = df.groupBy('customer_id').count()
transactions_per_customer.show()
```

9. **Find All Transactions That Occurred on the Same Day as a Withdrawal Greater Than $1,000**
- Filter the data to show all transactions that occurred on the same day as a withdrawal of more than $1,000.
```python
df_new = df.filter((df['transaction_type'] == 'Withdrawal') & (df['amount'] > 1000))
df_new.show()
```

10. **Create a New Column to Classify Transactions as "High" or "Low" Value**
- Add a new column transaction_value that classifies a transaction as "High" if the amount is greater than $5,000, otherwise classify it as "Low."
```python
from pyspark.sql.functions import when

df = df.withColumn('transaction_value', when(df['amount'] > 5000, 'High').otherwise('Low'))
df.show()
```

## Dataset: Health & Fitness Tracker Data
This dataset contains information about users' daily health and fitness activities,
including steps taken, calories burned, hours of sleep, and workout types.

### Sample Data:
```csv
user_id,date,steps,calories_burned,hours_of_sleep,workout_type
1,2023-09-01,12000,500,7,Cardio
2,2023-09-01,8000,400,6.5,Strength
3,2023-09-01,15000,650,8,Yoga
1,2023-09-02,10000,450,6,Cardio
2,2023-09-02,9500,500,7,Cardio
3,2023-09-02,14000,600,7.5,Strength
1,2023-09-03,13000,550,8,Yoga
2,2023-09-03,12000,520,6.5,Yoga
3,2023-09-03,16000,700,7,Cardio
```

### Exercises:
**Setting up the environment:**
```python
import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('Practice').getOrCreate()

file_name = '/content/sample_data/3_health_and_fitness_tracker.csv'
df = spark.read.csv(file_name, header=True, inferSchema=True)
df.show()
```

1. **Find the Total Steps Taken by Each User**
- Group the data by user_id and calculate the total steps taken by each user across all days.
```python
from pyspark.sql.functions import sum

total_steps_by_user = df.groupBy('user_id').agg(sum('steps').alias('total_steps'))
total_steps_by_user.show()
```

2. **Filter Days with More Than 10,000 Steps**
- Filter the dataset to show only the days where the user took more than 10,000 steps.
```python
days_with_more_than_10k_steps = df.filter(df['steps'] > 10000)
days_with_more_than_10k_steps.show()
```

3. **Calculate the Average Calories Burned by Workout Type**
- Group the data by workout_type and calculate the average calories burned for each workout type.
```python
from pyspark.sql.functions import avg

average_calories_by_workout = df.groupBy('workout_type').agg(avg('calories_burned').alias('average_calories'))
average_calories_by_workout.show()
```

4. **Identify the Day with the Most Steps for Each User**
- For each user, find the day when they took the most steps.
```python
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number

window = Window.partitionBy('user_id').orderBy(df['steps'].desc())

most_steps_for_each_user = df.withColumn('rank', row_number().over(window)).filter('rank == 1').select('user_id', 'date', 'steps')
most_steps_for_each_user.show()
```

5. **Find Users Who Burned More Than 600 Calories on Any Day**
- Filter the data to show only the users who burned more than 600 calories on any day.
```python
users_with_more_than_600_calories = df.filter(df['calories_burned'] > 600).select('user_id', 'date', 'calories_burned').distinct()
users_with_more_than_600_calories.show()
```

6. **Calculate the Average Hours of Sleep per User**
- Group the data by user_id and calculate the average hours of sleep for each user.
```python
average_hours_of_sleep_per_user = df.groupBy('user_id').agg(avg('hours_of_sleep').alias('average_hours_of_sleep'))
average_hours_of_sleep_per_user.show()
```

7. **Find the Total Calories Burned per Day**
- Group the data by date and calculate the total calories burned by all users combined for each day.
```python
total_calories_burned_per_day = df.groupBy('date').agg(sum('calories_burned').alias('total_calories_burned'))
total_calories_burned_per_day.show()
```

8. **Identify Users Who Did Different Types of Workouts**
- Identify users who participated in more than one type of workout.
```python
users_do_diff_workouts = df.select('user_id', 'workout_type').distinct().groupBy('user_id').count().filter('count > 1')
users_do_diff_workouts.show()
```

9. **Calculate the Total Number of Workouts per User**
- Group the data by user_id and count the total number of workouts completed by each user.
```python
total_workouts_per_user = df.groupBy('user_id').count().withColumnRenamed('count', 'total_workouts')
total_workouts_per_user.show()
```

10. **Create a New Column for "Active" Days**
- Add a new column called active_day that classifies a day as "Active" if the user took more than 10,000 steps, otherwise classify it as "Inactive."
```python
from pyspark.sql.functions import when

df = df.withColumn('active_day', when(df['steps'] > 10000, 'Active').otherwise('Inactive'))
df.show()
```

